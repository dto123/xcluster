{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([800, 128])\n",
      "torch.Size([200, 128])\n",
      "Epoch:  0 | train loss: 0.3693\n",
      "Epoch:  1 | train loss: 0.0945\n",
      "Epoch:  2 | train loss: 0.0853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:158: UserWarning: Couldn't retrieve source code for container of type AutoEncoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  3 | train loss: 0.0845\n",
      "Epoch:  4 | train loss: 0.0851\n",
      "Epoch:  5 | train loss: 0.0832\n",
      "Epoch:  6 | train loss: 0.0835\n",
      "Epoch:  7 | train loss: 0.0828\n",
      "Epoch:  8 | train loss: 0.0838\n",
      "Epoch:  9 | train loss: 0.0824\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "(800, 3)\n",
      "[[-0.3499271  -0.37041953  0.99974787]\n",
      " [-0.03650064 -0.2477209   0.99980569]\n",
      " [-0.24566032 -0.31638989  0.99977452]\n",
      " ..., \n",
      " [-0.2177778  -0.35899267  0.99978459]\n",
      " [-0.04199349 -0.41792053  0.99969965]\n",
      " [-0.06310819 -0.54048276  0.99977547]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as Data\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "import numpy as np\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "class AutoEncoderDataset(Dataset):\n",
    "    \"\"\"Dataset wrapper for autoencoding \"\"\"\n",
    "\n",
    "    def __init__(self, data=None):\n",
    "        self.data = data\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "def reduceData(data, output_dim):\n",
    "\n",
    "    #shuffle data and split it\n",
    "    np.random.shuffle(data)\n",
    "    \n",
    "    training, development = train_test_split(data, test_size=0.2)\n",
    "\n",
    "    #converting train data into torch\n",
    "    train_data = torch.from_numpy(training)\n",
    "    train_data = train_data.type(torch.FloatTensor)\n",
    "    points, input_dim = train_data.size()\n",
    "    print (train_data.size())\n",
    "\n",
    "    #converting development data into torch\n",
    "    dev_data = torch.from_numpy(development)\n",
    "    dev_data = dev_data.type(torch.FloatTensor)\n",
    "    dev_points, dev_input_dim = train_data.size()\n",
    "    print (dev_data.size())\n",
    "    # Hyper Parameters\n",
    "    EPOCH = 10\n",
    "    BATCH_SIZE = 64\n",
    "    LR = 0.005         # learning rate\n",
    "    DOWNLOAD_MNIST = False\n",
    "    N_TEST_IMG = 5\n",
    "\n",
    "    dataset = AutoEncoderDataset(train_data)\n",
    "    dev_dataset = AutoEncoderDataset(dev_data)\n",
    "    # Data Loader for easy mini-batch return in training, the image batch shape will be (50, 1, 28, 28)\n",
    "    train_loader = Data.DataLoader(dataset=dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    # Dev data loader\n",
    "    dev_loader = Data.DataLoader(dataset=dev_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    autoencoder = AutoEncoder(input_dim, output_dim)\n",
    "    optimizer = torch.optim.Adam(autoencoder.parameters(), lr=LR)\n",
    "    loss_func = nn.MSELoss()\n",
    "\n",
    "    bestLoss = math.inf\n",
    "    for epoch in range(EPOCH):\n",
    "        for step, (x, y) in enumerate(train_loader):\n",
    "            b_x = Variable(x.view(-1, input_dim))   # batch x, shape (batch, 28*28)\n",
    "            b_y = Variable(x.view(-1, input_dim))   # batch y, shape (batch, 28*28)\n",
    "\n",
    "            #b_label = Variable(y)               # batch label\n",
    "\n",
    "            encoded, decoded = autoencoder(b_x)\n",
    "\n",
    "            loss = loss_func(decoded, b_y)      # mean square error\n",
    "            optimizer.zero_grad()               # clear gradients for this training step\n",
    "            loss.backward()                     # backpropagation, compute gradients\n",
    "            optimizer.step()                    # apply gradients\n",
    "\n",
    "            if step % 100 == 0:\n",
    "                print('Epoch: ', epoch, '| train loss: %.4f' % loss.data[0])\n",
    "                \n",
    "                lossDev = 0\n",
    "                for step, (d_x, d_y) in enumerate(dev_loader):\n",
    "                    dev_x = Variable(d_x.view(-1, dev_input_dim))   \n",
    "                    \n",
    "                    encodedDev, decodedDev = autoencoder(dev_x)\n",
    "              \n",
    "                    lossDev += loss_func(decodedDev, dev_x).cpu().data.numpy()[0]\n",
    "                    \n",
    "                if lossDev < bestLoss:\n",
    "                    torch.save(autoencoder, \"model2.torch\")\n",
    "        \n",
    "                \n",
    "                \n",
    "                #run dev by using loss function and not calling zero_grad and step\n",
    "                #give autoencdoer test data and it returns a output vector and then compare input and output\n",
    "                #forward calculates weights and gradients, while backpropagation goes back and improves weights and gradients until they converge\n",
    "                #gradient goes in direction of max increase so to get minimum so we take negative direction\n",
    "                #torch.save()\n",
    "\n",
    "    # visualize in 3D plot\n",
    "    view_data = Variable(train_data.view(-1, input_dim))\n",
    "    encoded_data, _ = autoencoder(view_data)\n",
    "    return encoded_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        d_prime_1 = input_dim\n",
    "        self.encoder = nn.Sequential(\n",
    "            \n",
    "           \n",
    "            #nn.Linear(input_dim, output_dim),\n",
    "            #nn.Tanh(),\n",
    "            \n",
    "            \n",
    "            \n",
    "            #d_prime_1 = (input_dim+output_dim)/2\n",
    "            #d_prime_2 = input_dim\n",
    "\n",
    "            nn.Linear(input_dim, d_prime_1),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(d_prime_1, output_dim),\n",
    "            nn.Tanh(),\n",
    "            \n",
    "            \n",
    "            \n",
    "            #nn.Linear(128, 64),\n",
    "            #nn.Tanh(),\n",
    "            #nn.Linear(64, 12),\n",
    "            #nn.Tanh(),\n",
    "            #nn.Linear(12, output_dim),   # compress to 3 features which can be visualized in plt\n",
    "            \n",
    "            #one hidden layer\n",
    "            \n",
    "            #nn.Linear(input_dim, output_dim),\n",
    "            #nn.Tanh(),\n",
    "            \n",
    "            \n",
    "            \n",
    "        )\n",
    "        \n",
    "      \n",
    "        self.decoder = nn.Sequential(\n",
    "           \n",
    "            #d_prime = (output_dim + input_dim)/2\n",
    "            #nn.Linear(output_dim, input_dim),\n",
    "            #nn.Tanh(),\n",
    "            \n",
    "            \n",
    "            \n",
    "            #d_prime_1 = (input_dim+output_dim)/2\n",
    "            #d_prime_2 = input_dim\n",
    "\n",
    "            nn.Linear(output_dim, d_prime_1),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(d_prime_1, input_dim),\n",
    "            nn.Tanh(),\n",
    "            \n",
    "            \n",
    "            \n",
    "            #nn.Linear(12, 64),\n",
    "            #nn.Tanh(),\n",
    "            #nn.Linear(64, 128),\n",
    "            #nn.Tanh(),\n",
    "            #nn.Linear(128, input_dim),\n",
    "            #nn.Sigmoid(),       # compress to a range (0, 1)\n",
    "            \n",
    "            \n",
    "            #nn.Linear(output_dim, input_dim),\n",
    "            #nn.Tanh(),\n",
    "        \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded\n",
    "\n",
    "encoded_data = reduceData(np.random.random_sample((1000,128)), 3)\n",
    "print(type(encoded_data))\n",
    "encoded_data =encoded_data.data.numpy()\n",
    "print(encoded_data.shape)\n",
    "print(encoded_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
